---
title: "The Office 1"
author: "Your name"
output: github_document
---

```{r load-packages, message = FALSE}
# credit: Modified from Data Science in a Box 
# credit: Inspired by Alison Hill's Alzheimer's tutorial

  library(tidymodels)
  library(stringr)
  library(schrute)
  library(lubridate)
```

In this two-part exercise we will use `theoffice` data from the [**schrute**](https://bradlindblad.github.io/schrute/) package to predict IMDB scores for episodes of The Office. In the first part, we'll work together to prepare the data, specify a model, and create a recipe. In the second part, you'll fit and evaluate the model on your own for homework.

```{r}
  glimpse(theoffice)
```

Fix `air_date` for later use.

```{r}
  theoffice <- theoffice %>%
    mutate(air_date = ymd(as.character(air_date)))
```

We will

-   engineer features based on episode scripts
-   train a model
-   perform cross validation
-   make predictions

```{r}
  theoffice %>%
    distinct(season, episode)
```

### Exercise 1

Calculate the percentage of lines spoken by Jim, Pam, Michael, and Dwight for each episode of The Office.

```{r lines}
  office_lines <- theoffice %>%
    group_by(season, episode) %>%
    mutate(
      n_lines = n(),
      lines_jim = sum(character=="Jim") / n_lines,
      lines_pam = sum(character=="Pam") / n_lines,
      lines_dwight = sum(character=="Dwight") / n_lines,
      lines_michael = sum(character=="Michael") / n_lines
    ) %>%
    ungroup() %>%
    select(season, episode, episode_name, contains("lines")) %>%
    distinct(season, episode, episode_name, .keep_all = TRUE)
```

### Exercise 2

Identify episodes that touch on Halloween, Valentine's Day, and Christmas.

```{r special-episodes}
  theoffice <- theoffice %>%
    mutate(text = tolower(text))
  
  halloween_episodes <- theoffice %>%
    filter(str_detect(text, "halloween")) %>%
    count(episode_name) %>%
    filter(n > 1) %>%
    mutate(halloween = 1) %>%
    select(-n)
  
  valentine_episodes <- theoffice %>%
    filter(str_detect(text, "valentine")) %>%
    count(episode_name) %>%
    filter(n > 1) %>%
    mutate(valentine = 1) %>%
    select(-n)
  
  christmas_episodes <- theoffice %>%
    filter(str_detect(text, "christmas")) %>%
    count(episode_name) %>%
    filter(n > 1) %>%
    mutate(christmas = 1) %>%
    select(-n)
```

### Exercise 3

Put together a modeling dataset that includes features you've engineered. Also add an indicator variable called `michael` which takes the value `1` if Michael Scott (Steve Carrell) was there, and `0` if not. Note: Michael Scott (Steve Carrell) left the show at the end of Season 7.

```{r office-df}
  office_df <- theoffice %>%
    select(season, episode, episode_name, imdb_rating, total_votes, air_date) %>%
    distinct(season, episode, .keep_all = TRUE) %>%
    left_join(halloween_episodes, by = "episode_name") %>%
    left_join(valentine_episodes, by = "episode_name") %>%
    left_join(christmas_episodes, by = "episode_name") %>%
    replace_na(list(halloween = 0, valentine = 0, christmas = 0)) %>%
    mutate(michael = case_when(
        season > 7 ~ 0,
        TRUE ~ 1
      )) %>%
    mutate(across(halloween:michael, as.factor)) %>%
    left_join(office_lines, by=c("season", "episode", "episode_name"))
```

### Exercise 4 

Use `initial_split()`, `training()`, and `testing()` to split `office_df` into training (75%) and test (25%) sets.
 
```{r split}
  set.seed(1122)
  office_split <- ____(office_df)
  office_train <- ____(office_split)
  office_test <- ____(office_split)
```

### Exercise 5

Specify a linear regression model (`lm` engine). Visit https://www.tidymodels.org/find/parsnip/ and search for the details.

```{r model}
# linear regression model
  linreg_mod <- 
  
  linreg_mod # leave this so it prints
```

### Exercise 6

Create a recipe that updates the role of `episode_name` to not be a predictor, removes `air_date` as a predictor, uses `season` as a factor, and removes all zero variance predictors. Visit https://recipes.tidymodels.org/reference/index.html to find the correct `step_()` functions.

```{r recipe}
  rec <- ____(____ ~ ., data = ____) %>%  # the ~ . means everything else
    ____(episode_name, new_role = "id") %>%
    step_rm(____) %>%
    ____(all_nominal(), -episode_name) %>%
    ____(____)

  rec # leave this so it prints
```

### Exercise 7

Build a workflow for fitting the linear regression specified earlier and using the recipe you developed to preprocess the data.

```{r workflow}
  linreg_wflow <- ____() %>%
    add_model(____) %>%
    ____(rec)

  linreg_wflow # leave this so it prints
```

### Exercise 8 

Use `vfold_cv()` to split the data (which one??) into 10 folds.

```{r vfold}
  folds <- vfold_cv(____, v = 5)  # just 5 to save computation time 

  folds # leave this so it prints
```


### Exercise 9

Use `fit_resamples()` and `collect_metrics()` with your `linreg_wflow` to perform 5-fold cross validation on the training data.

```{r fit}
  linreg_results <- ____ %>%
    ____(folds) %>%
    ____()

  linreg_results # leave this so it prints
```

We'll look at [root mean square error](https://www.tmwr.org/performance.html) (RMSE). When comparing models, you could select the one that minimizes RMSE.

### Exercise 10

1. Create a new parsnip random forest model called `rf_mod` which will learn an ensemble of classification trees from the training data. Use the `ranger` engine.
2. Create a workflow object called `rf_wflow` that uses this model and the recipe you created earlier.
3. Leave `tune()` empty for the `mtry` and `min_n` hyperparameters. This will tell tidymodels to search for the optimal combination of values to improve predictions. 

```{r rrf}
# create the model
#install.packages("ranger")
  rforest_mod <- rand_forest(mtry = tune(),
                             min_n = tune()) %>% 
    ____("ranger") %>% 
    ____("regression")
  
# create a workflow
  rforest_wflow <- ____() %>%
    add_model(____) %>%
    ____(rec)
  
# fit the model to the training data
  rforest_results <- rforest_wflow %>% 
    tune_grid(resamples = folds,
              control = control_grid(verbose = TRUE)) 
```

### Exercise 11

Inspect `linreg_results` and `linreg_results`. Which model (`xxxxxx`) performed the best? (remember, you want the model that minimizes error, so lower is better)

```{r}
  linreg_results %>% collect_metrics()

# remember with the random forest model you let tidymodels try different 
# combinations of the hyperparameters mtry and min_n
# the top result will be the best of all combinations
  rforest_results %>%  
    collect_metrics() %>% 
    filter(.metric=="rmse") %>%
    arrange(mean)
```

Use `select_best()`, `finalize_workflow()`, and `last_fit()` to take the best combination of hyper-parameters from the best model and use them to predict the test set.

```{r test}
  office_best <- xxxxxx_results %>% 
    select_best(metric = "RMSE")

  office_best # leave this so it prints

  last_workflow <- xxxxxx_wflow %>%
    finalize_workflow(office_best) 
  
  last_workflow # leave this so it prints
  
  last_results <- last_workflow %>% 
    last_fit(split = office_split) %>% # this uses the test set in the split
    collect_metrics()
  
  last_results # leave this so it prints
```